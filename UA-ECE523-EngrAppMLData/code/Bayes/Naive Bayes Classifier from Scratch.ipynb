{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "this is a tutorial for how to implement an sklearn compatible gaussian naive bayes classifier from scratch. Above, we've imported the libraries we'll need to implement the classifier as well as the built in sklearn GaussianNB classifier we'll use as a reference for the performance of our classifier. If you haven't already, make sure you run the cell above to import the libraries.\n",
    "\n",
    "# Getting Some Data\n",
    "We'll use data from the ECE 523 github page for this demo. The datafiles can be found [here](https://github.com/gditzler/UA-ECE-523-Sp2018/tree/master/data). Note that in the function below we use the path to the raw data on github for downloading. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_course_data(dataset_name = \"abalone.csv\"):\n",
    "    \"\"\"\n",
    "    loads data from the ece_523 github page.\n",
    "    :param dataset_name: filename of the dataset to load (default is \"abalone.csv\"\n",
    "    :return: numpy array containing the csv data\n",
    "    \"\"\"\n",
    "    data_url_format = \"https://raw.githubusercontent.com/gditzler/UA-ECE-523-Sp2018/master/data/{0:s}\"\n",
    "\n",
    "    data = pd.read_csv(data_url_format.format(dataset_name))\n",
    "\n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Out!\n",
    "let's just load the default abalone.csv dataset and take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4176, 9)\n",
      "[[-1.15421   -1.44881   -1.43976   ... -1.20508   -1.21284    0.       ]\n",
      " [ 0.0537917  0.0500271  0.122116  ... -0.356647  -0.207114   1.       ]\n",
      " [-1.15421   -0.699393  -0.432097  ... -0.607527  -0.602222   1.       ]\n",
      " ...\n",
      " [-1.15421    0.632909   0.676328  ...  0.975296   0.496895   1.       ]\n",
      " [ 0.0537917  0.841081   0.777094  ...  0.73354    0.41069    1.       ]\n",
      " [-1.15421    1.54887    1.48246   ...  1.78723    1.84026    2.       ]]\n"
     ]
    }
   ],
   "source": [
    "myData = load_course_data()\n",
    "print(myData.shape)\n",
    "print(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see we get a numpy array containing 4176 rows of data in 9 columns. The last column contains the labels. Typically SKLearn Classifiers take the features and labels as separate arguments, so let's write a function to break the data down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    \"\"\"\n",
    "    splits a n-by-m numpy array containing n datapoints with m - 1 features, and a last column containing labels\n",
    "    into data (X) and labels (Y)\n",
    "    :param data: the numpy array to split up\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    Y = data[:, -1]\n",
    "    X = data[:, 0:data.shape[1] - 1]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4176, 8)\n",
      "[[-1.15421   -1.44881   -1.43976   ... -1.17077   -1.20508   -1.21284  ]\n",
      " [ 0.0537917  0.0500271  0.122116  ... -0.463444  -0.356647  -0.207114 ]\n",
      " [-1.15421   -0.699393  -0.432097  ... -0.64816   -0.607527  -0.602222 ]\n",
      " ...\n",
      " [-1.15421    0.632909   0.676328  ...  0.74847    0.975296   0.496895 ]\n",
      " [ 0.0537917  0.841081   0.777094  ...  0.773248   0.73354    0.41069  ]\n",
      " [-1.15421    1.54887    1.48246   ...  2.64068    1.78723    1.84026  ]]\n",
      "(2088, 8)\n",
      "[[-1.15421    0.882716   0.978626  ...  1.23053    0.601258   0.942289 ]\n",
      " [-1.15421   -2.07333   -2.09474   ... -1.45235   -1.42859   -1.50019  ]\n",
      " [ 0.0537917  0.54964    0.777094  ...  0.419586   0.523713   1.30148  ]\n",
      " ...\n",
      " [ 0.0537917 -0.0332418  0.222882  ... -0.258455  -0.210681  -0.02752  ]\n",
      " [ 0.0537917  0.424737   0.52518   ... -0.118792   0.240903   0.428649 ]\n",
      " [ 0.0537917  0.591275   0.575563  ...  0.712428   0.57845    0.475344 ]]\n",
      "(2088,)\n",
      "[2. 0. 2. ... 2. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "data, labels = split_data(myData)\n",
    "# let's split the data into training and testing data\n",
    "X, X_t, Y, Y_t = train_test_split(\n",
    "        data,\n",
    "        labels,\n",
    "        test_size=.5,\n",
    "        random_state=42\n",
    "    ) \n",
    "print(data.shape)\n",
    "print(data)\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Naive Bayes\n",
    "for a Naive Bayes Classifier $P(Y_k | X) = \\frac{P(Y_k) \\times P(X | Y_k)}{P(X)}$ or the $Posterior = \\frac{prior \\times liklihood}{evidence}$ now $evidence$ can be hard to find, but fortunately, it's also the least important, since it's the same for all $Y_i$, and therefore scales all of the posterior probablities proportionally. So we can simply leave it out. That leaves us with $P(Y_k | X) \\propto P(Y_k) \\times P(X | Y_k)$ as the fundamental equation of our naive bayes classifier.\n",
    "\n",
    "## Calculating the prior probability\n",
    "to create our classifier, it's easiest to start by calculating the prior probabilities for all of the classes in the dataset. $P(Y_k) = \\frac{number\\ of\\ occurences\\ of\\ Y_k\\ in\\ the\\ dataset}{total\\ size\\ of\\ the\\ data\\ set}$\n",
    "\n",
    "## Calculating the liklihood or class conditional probability\n",
    "This is where the \"Gaussian\" and \"Naive\" from the name of the classifier come into play. First, we're going to assume the class conditional probabilities are normally distributed, second we're going to assume that they're independent from one another. This makes our posterior probability equation $P(Y_k | X) \\propto P(Y_k)\\prod_{i=1}^{n-features}P(X_i | Y_k)$. For model training purposes this means for each class we're going to want the mean and variance for each feature that is part of a data point in that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X,Y):\n",
    "        # store the number of classes\n",
    "        n_classes = np.unique(Y).size\n",
    "        print(\"n_classes:{0:d}\".format(n_classes))\n",
    "\n",
    "        # store the number of occurences of each class in the training data\n",
    "        class_count = np.zeros(n_classes)\n",
    "        for label in range(0,n_classes):\n",
    "            class_count[label] = Y[Y == label].size\n",
    "        print((\"class_count:\" + np.array_str(class_count)))\n",
    "\n",
    "        # compute the prior probability of each class from the training data\n",
    "        prior = np.zeros(n_classes)\n",
    "        for idx, value in enumerate(class_count):\n",
    "            prior[idx] = value / np.sum(class_count)\n",
    "\n",
    "        print(\"prior:\" + np.array_str(prior))\n",
    "        print(\"sum or prior = {0:f}\".format(np.sum(prior)))\n",
    "\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        print(\"n_features:{0:d}\".format(n_features))\n",
    "\n",
    "        # compute the mean and variance of each feature per class\n",
    "        mean = np.zeros((n_classes, n_features))\n",
    "        variance = np.zeros(mean.shape)\n",
    "        for lbl in range(0, n_classes):\n",
    "            # group training data by label\n",
    "            X_lbl = X[Y == lbl, :]\n",
    "            mean[lbl] = np.mean(X_lbl, axis = 0)\n",
    "            variance[lbl] = np.var(X_lbl, axis = 0)\n",
    "\n",
    "        print(\"mean:\\n\" + np.array_str(mean))\n",
    "        print(\"variance:\\n\" + np.array_str(variance))\n",
    "\n",
    "        return n_classes, class_count, prior, n_features, mean, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's give it a try. I've thrown some print statements into the function so you can see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes:3\n",
      "class_count:[679. 683. 726.]\n",
      "prior:[0.32519157 0.32710728 0.34770115]\n",
      "sum or prior = 1.000000\n",
      "n_features:8\n",
      "mean:\n",
      "[[ 0.55727182 -0.86169938 -0.87419106 -0.75990272 -0.81258685 -0.72754601\n",
      "  -0.79575554 -0.85574846]\n",
      " [-0.18851684  0.28544751  0.26728962  0.19620967  0.17310374  0.22689534\n",
      "   0.1938192   0.12234517]\n",
      " [-0.37882634  0.59838586  0.61491132  0.60300419  0.6472      0.52253034\n",
      "   0.6162865   0.72332738]]\n",
      "variance:\n",
      "[[1.00804946 0.85582724 0.81359802 1.46653394 0.3747472  0.4277836\n",
      "  0.37245287 0.3223442 ]\n",
      " [0.86214067 0.51027803 0.49020357 0.55115871 0.63525016 0.75351929\n",
      "  0.69678389 0.51698034]\n",
      " [0.63292535 0.4735234  0.46277384 0.47199598 0.8959942  0.99677558\n",
      "  0.9276522  0.88113948]]\n"
     ]
    }
   ],
   "source": [
    "n_classes, class_count, prior, n_features, mean, variance = fit_model(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "to make predictions with this model, we'll be finding the label with the maximum posterior probability as produced by the equation above. Howver, we'll be making one small tweak. We'll be using the log probabilities. This is useful for preventing floating point underflows, because otherwise we could potentially be multiplying a lot of numbers that are $<1$ together. Note that this makes the equation for the posterior: $log(P(Y_k | x)) \\propto log(P(Y_k)) + \\sum_{i=1}^{n-features}log(P(X_i | Y_k)$. To start with we'll need a function to compute $\\sum log(P(X_i|Y_k)$ from the gaussian distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_liklihood:class_log_liklihood:\n",
      "[[-27.55305659  -8.78586156  -7.01193884]\n",
      " [-11.17139943 -27.94739505 -35.68392261]\n",
      " [-24.85525792  -9.25451132  -7.20786552]\n",
      " ...\n",
      " [ -9.27791008  -5.95421557  -8.09787932]\n",
      " [-13.83868839  -5.70351967  -6.59390769]\n",
      " [-18.53769462  -6.17448008  -6.40747287]]\n"
     ]
    }
   ],
   "source": [
    "def log_liklihood(X_test, n_classes, mean, variance):\n",
    "    class_log_liklihoods = []\n",
    "    for idx in np.arange(0, n_classes):\n",
    "            liklihood = - 0.5 * np.sum(np.log(2 * np.pi * variance[idx,:]))\n",
    "            liklihood += -0.5 * np.sum(np.square((X - mean[idx, :])) / ( variance[idx,:]) , 1)\n",
    "            class_log_liklihoods.append(liklihood)\n",
    "\n",
    "    class_log_liklihoods = np.array(class_log_liklihoods).T\n",
    "    print(\"log_liklihood:class_log_liklihood:\\n\" + np.array_str(class_log_liklihoods))\n",
    "    return class_log_liklihoods\n",
    "\n",
    "l_liklihood = log_liklihood(X_t, n_classes, mean, variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we'll combine these with the posterior to get our prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probas\n",
      "[[-28.67639741  -9.90332865  -8.06835078]\n",
      " [-12.29474025 -29.06486214 -36.74033454]\n",
      " [-25.97859874 -10.37197841  -8.26427745]\n",
      " ...\n",
      " [-10.4012509   -7.07168266  -9.15429126]\n",
      " [-14.96202921  -6.82098676  -7.65031963]\n",
      " [-19.66103544  -7.29194716  -7.4638848 ]]\n"
     ]
    }
   ],
   "source": [
    "def predict_log_proba(prior, log_liklihood):\n",
    "    log_prior = np.log(prior)\n",
    "    \n",
    "    log_probas = np.zeros(log_liklihood.shape)\n",
    "    \n",
    "    for idx, row in enumerate(log_liklihood):\n",
    "        log_probas[idx] = np.add(row, log_prior)\n",
    "        \n",
    "    print(\"log_probas\\n\" + np.array_str(log_probas))\n",
    "    return log_probas\n",
    "\n",
    "log_probas = predict_log_proba(prior, l_liklihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Prediction:\n",
    "the last thing to do is select the column corresponding to the label with the maximum posterior probability..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [2. 0. 2. ... 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 0., 2., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(log_probas):\n",
    "    predictions = np.zeros(log_probas.shape[0])\n",
    "    for idx, row in enumerate(log_probas):\n",
    "        predictions[idx] = np.argmax(row)\n",
    "\n",
    "    print(\"predictions: \" + np.array_str(predictions))\n",
    "    return predictions\n",
    "\n",
    "predict(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An SKLearn Classifier Comparison\n",
    "Below I've wrapped the functions from the examples above into an sklearn compatible classifier, complete with \"score\" function that can be used in k-fold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class GaussianNaiveBayesFromScratch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.n_classes  = None\n",
    "        self.prior = None\n",
    "        self.n_features = None\n",
    "        self.class_count = None\n",
    "        self.mean = None\n",
    "        self.variance = None\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        # store the number of classes\n",
    "        self.n_classes = np.unique(Y).size\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:n_classes:{0:d}\".format(self.n_classes))\n",
    "\n",
    "        # store the number of occurences of each class in the training data\n",
    "        self.class_count = np.zeros(self.n_classes)\n",
    "        for label in range(0,self.n_classes):\n",
    "            self.class_count[label] = Y[Y == label].size\n",
    "        #logging.debug((\"GaussianNaiveBayesFromScratch:class_count:\" + np.array_str(self.class_count)))\n",
    "\n",
    "        # compute the prior probability of each class from the training data\n",
    "        self.prior = np.zeros(self.n_classes)\n",
    "        for idx, value in enumerate(self.class_count):\n",
    "            self.prior[idx] = value / np.sum(self.class_count)\n",
    "\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:prior:\" + np.array_str(self.prior))\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:sum or prior = {0:f}\".format(np.sum(self.prior)))\n",
    "\n",
    "\n",
    "        self.n_features = X.shape[1]\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:n_features:{0:d}\".format(self.n_features))\n",
    "\n",
    "        # compute the mean and variance of each feature per class\n",
    "        self.mean = np.zeros((self.n_classes, self.n_features))\n",
    "        self.variance = np.zeros(self.mean.shape)\n",
    "        for lbl in np.arange(0, self.n_classes):\n",
    "            # group training data by label\n",
    "            X_lbl = X[Y == lbl, :]\n",
    "            mean = np.mean(X_lbl, axis = 0)\n",
    "            variance = np.var(X_lbl, axis = 0)\n",
    "\n",
    "            self.mean[lbl] = mean\n",
    "            self.variance[lbl] = variance\n",
    "\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:mean:\\n\" + np.array_str(self.mean))\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:variance:\\n\" + np.array_str(self.variance))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _log_liklihood(self, X):\n",
    "\n",
    "        class_log_liklihoods = []\n",
    "        for idx in np.arange(0, self.n_classes):\n",
    "            liklihood = - 0.5 * np.sum(np.log(2 * np.pi * self.variance[idx,:]))\n",
    "            liklihood += -0.5 * np.sum(np.square((X - self.mean[idx, :])) / ( self.variance[idx,:]) , 1)\n",
    "            class_log_liklihoods.append(liklihood)\n",
    "\n",
    "        class_log_liklihoods = np.array(class_log_liklihoods).T\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:_log_liklihood:class_log_liklihood:\\n\" + np.array_str(class_log_liklihoods))\n",
    "        return class_log_liklihoods\n",
    "\n",
    "    @property\n",
    "    def _log_prior(self):\n",
    "        log_prior = np.log(self.prior)\n",
    "\n",
    "        #logging.debug(\"GaussianNaiveBayesFromScratch:_log_prior:log_prior\\n\" + np.array_str(log_prior))\n",
    "\n",
    "        return log_prior\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        log_liklihood = self._log_liklihood(X)\n",
    "\n",
    "        log_probas = np.zeros(log_liklihood.shape)\n",
    "\n",
    "        for idx, row in enumerate(log_liklihood):\n",
    "            # this is the sum log(p(Y)) + log(p(X | Y))\n",
    "            log_probas[idx] = np.add(row , self._log_prior)\n",
    "\n",
    "        #logging.debug(\"GaussinaNaiveBayesFromScratch:predict_log_probas:log_probas:\\n\" + np.array_str(log_probas))\n",
    "\n",
    "        return log_probas\n",
    "\n",
    "    def predict(self, X):\n",
    "        prob = self.predict_log_proba(X)\n",
    "\n",
    "        predictions = np.zeros(prob.shape[0])\n",
    "        for idx, row in enumerate(prob):\n",
    "            predictions[idx] = np.argmax(row)\n",
    "\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        predicted = self.predict(X)\n",
    "\n",
    "        correct = predicted[predicted == Y]\n",
    "\n",
    "        return correct.shape[0] / Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how this compares to the built in SKLearn Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def compare_classifiers(data_set = \"abalone.csv\"):\n",
    "   \n",
    "    data = load_course_data(data_set)\n",
    "    data, labels = split_data(data)\n",
    "    \n",
    "    GNB_from_scratch = GaussianNaiveBayesFromScratch()\n",
    "    GNB_fs_scores = cross_val_score(GNB_from_scratch, data, labels, cv=5)\n",
    "    \n",
    "    print(\"GaussianNaiveBayesFromScratch 5-fold cross validation score: {0:f}\".format(np.mean(GNB_fs_scores)))\n",
    "    \n",
    "    sk_GNB = GaussianNB()\n",
    "    sk_scores = cross_val_score(sk_GNB, data, labels, cv=5)\n",
    "    \n",
    "    print(\"GaussianNB 5-fold cross validation score: {0:f}\".format(np.mean(sk_scores)))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some of the different data-sets on the github page by changing out the filename in the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNaiveBayesFromScratch 5-fold cross validation score: 0.809582\n",
      "GaussianNB 5-fold cross validation score: 0.809582\n"
     ]
    }
   ],
   "source": [
    "compare_classifiers(\"adult_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
